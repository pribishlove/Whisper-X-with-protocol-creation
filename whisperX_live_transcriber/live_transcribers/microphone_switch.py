import threading
import queue
import numpy as np
import sounddevice as sd
import tempfile
import os
import scipy.io.wavfile as wav
import whisperx
import torch
import re

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
SAMPLERATE = 16000
CHUNK_DURATION = 2  # —Å–µ–∫—É–Ω–¥
STOP_PHRASE = "—Å—Ç–æ–ø"
language_code = "ru"
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "turbo"
compute_type = "float32"

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ---
audio_queue = queue.Queue()
stop_event = threading.Event()
full_text = ""
buffer_text = ""

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ---
print("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ WhisperX...")
model = whisperx.load_model(
    whisper_arch=model_name,
    device=device,
    language=language_code,
    compute_type=compute_type
)
print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

# --- –§—É–Ω–∫—Ü–∏—è –≤—ã–±–æ—Ä–∞ –∞—É–¥–∏–æ—É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ ---
def choose_audio_device():
    print("\nüéõÔ∏è –î–æ—Å—Ç—É–ø–Ω—ã–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –≤–≤–æ–¥–∞:")
    devices = sd.query_devices()
    input_devices = [(i, d) for i, d in enumerate(devices) if d['max_input_channels'] > 0]

    for i, dev in input_devices:
        print(f"[{i}] {dev['name']} ({dev['hostapi']})")

    while True:
        try:
            idx = int(input("üîò –í—ã–±–µ—Ä–∏—Ç–µ —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ –≤–≤–æ–¥–∞ (–ø–æ –Ω–æ–º–µ—Ä—É): "))
            if any(i == idx for i, _ in input_devices):
                return idx
        except ValueError:
            pass
        print("‚ùå –ù–µ–≤–µ—Ä–Ω—ã–π –≤—ã–±–æ—Ä, –ø–æ–ø—Ä–æ–±—É–π—Ç–µ —Å–Ω–æ–≤–∞.")

# --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π ---
def extract_sentences(text):
    matches = list(re.finditer(r'[^.?!]+[.?!]', text))
    sentences = [match.group().strip() for match in matches]
    remainder = text[matches[-1].end():].strip() if matches else text.strip()
    return sentences, remainder

# --- –ó–∞–ø–∏—Å—å –∞—É–¥–∏–æ —á–∞–Ω–∫–æ–≤ ---
def record_audio():
    while not stop_event.is_set():
        chunk = sd.rec(int(SAMPLERATE * CHUNK_DURATION),
                       samplerate=SAMPLERATE, channels=1, dtype='float32')
        sd.wait()
        audio_queue.put(np.squeeze(chunk))

# --- –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —á–∞–Ω–∫–æ–≤ ---
def transcribe_loop():
    global full_text, buffer_text
    while not stop_event.is_set() or not audio_queue.empty():
        try:
            chunk = audio_queue.get(timeout=1)
        except queue.Empty:
            continue

        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
            wav.write(tmpfile.name, SAMPLERATE, (chunk * 32767).astype(np.int16))
            tmp_path = tmpfile.name

        audio = whisperx.load_audio(tmp_path)
        result = model.transcribe(audio)
        os.remove(tmp_path)

        segments = result.get("segments", [])
        current_raw_text = " ".join(seg["text"].strip().lower() for seg in segments if seg["text"].strip())

        if not current_raw_text or "no active speech" in current_raw_text.lower():
            continue

        print(f"\nüéß –¢–µ–∫—É—â–∏–π —á–∞–Ω–∫: {current_raw_text}")

        combined_text = buffer_text + " " + current_raw_text
        sentences, buffer_text = extract_sentences(combined_text)

        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and sentence not in full_text:
                capitalized = sentence[0].upper() + sentence[1:] if sentence else ""
                full_text += capitalized + " "
                print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ: {capitalized}")

        if STOP_PHRASE in current_raw_text:
            print("üõë –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Ñ—Ä–∞–∑–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.")
            stop_event.set()
            break

    if buffer_text.strip():
        capitalized = buffer_text.strip()[0].upper() + buffer_text.strip()[1:]
        full_text += capitalized + " "
        print(f"üìù –î–æ–±–∞–≤–ª–µ–Ω –æ—Å—Ç–∞—Ç–æ–∫ –±—É—Ñ–µ—Ä–∞: {capitalized}")

    print("\nüìã –ü–æ–ª–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:")
    final_text = full_text.strip()
    if final_text:
        final_text = final_text[0].upper() + final_text[1:]
    print(final_text)

# --- –í—ã–±–æ—Ä —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º ---
selected_device_index = choose_audio_device()
sd.default.device = (selected_device_index, None)

# --- –ó–∞–ø—É—Å–∫ ---
print("\nüéôÔ∏è –°–∫–∞–∂–∏—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å... (–ü—Ä–æ–∏–∑–Ω–µ—Å–∏—Ç–µ '—Å—Ç–æ–ø' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è)\n")

rec_thread = threading.Thread(target=record_audio)
trans_thread = threading.Thread(target=transcribe_loop)

rec_thread.start()
trans_thread.start()

rec_thread.join()
trans_thread.join()
print("‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ.")
