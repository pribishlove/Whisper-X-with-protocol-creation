import threading
import queue
import numpy as np
import sounddevice as sd
import tempfile
import os
import scipy.io.wavfile as wav
import whisperx
import torch
import re

# --- –ù–∞—Å—Ç—Ä–æ–π–∫–∏ ---
SAMPLERATE = 16000
CHUNK_DURATION = 2  # —Å–µ–∫—É–Ω–¥
STOP_PHRASE = "—Å—Ç–æ–ø"
language_code = "ru"
device = "cuda" if torch.cuda.is_available() else "cpu"
model_name = "turbo"
compute_type = "float32"

# --- –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è ---
audio_queue = queue.Queue()
stop_event = threading.Event()
full_text = ""
buffer_text = ""

# --- –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ ---
print("üöÄ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ WhisperX...")
model = whisperx.load_model(
    whisper_arch=model_name,
    device=device,
    language=language_code,
    compute_type=compute_type
)
print("‚úÖ –ú–æ–¥–µ–ª—å –∑–∞–≥—Ä—É–∂–µ–Ω–∞.")

# --- –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã—Ö –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π ---
def extract_sentences(text):
    # –ò—â–µ–º –∑–∞–≤–µ—Ä—à—ë–Ω–Ω—ã–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    matches = list(re.finditer(r'[^.?!]+[.?!]', text))
    sentences = [match.group().strip() for match in matches]
    
    # –û—Å—Ç–∞—Ç–æ–∫ ‚Äî –≤—Å—ë –ø–æ—Å–ª–µ –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –Ω–∞–π–¥–µ–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è
    if matches:
        last_index = matches[-1].end()
        remainder = text[last_index:].strip()
    else:
        remainder = text.strip()
    
    return sentences, remainder

# --- –ó–∞–ø–∏—Å—å –∞—É–¥–∏–æ —á–∞–Ω–∫–æ–≤ ---
def record_audio():
    while not stop_event.is_set():
        chunk = sd.rec(int(SAMPLERATE * CHUNK_DURATION),
                       samplerate=SAMPLERATE, channels=1, dtype='float32')
        sd.wait()
        audio_queue.put(np.squeeze(chunk))

# --- –¢—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è —á–∞–Ω–∫–æ–≤ ---
def transcribe_loop():
    global full_text, buffer_text
    while not stop_event.is_set() or not audio_queue.empty():
        try:
            chunk = audio_queue.get(timeout=1)
        except queue.Empty:
            continue

        with tempfile.NamedTemporaryFile(suffix=".wav", delete=False) as tmpfile:
            wav.write(tmpfile.name, SAMPLERATE, (chunk * 32767).astype(np.int16))
            tmp_path = tmpfile.name

        audio = whisperx.load_audio(tmp_path)
        result = model.transcribe(audio)
        os.remove(tmp_path)

        segments = result.get("segments", [])
        current_raw_text = " ".join(seg["text"].strip().lower() for seg in segments if seg["text"].strip())

        # –ü—Ä–æ–ø—É—Å–∫ –ø—É—Å—Ç—ã—Ö –∏–ª–∏ –Ω–µ–∏–Ω—Ñ–æ—Ä–º–∞—Ç–∏–≤–Ω—ã—Ö —á–∞–Ω–∫–æ–≤
        if not current_raw_text or "no active speech" in current_raw_text or "No active speech found in audio" in current_raw_text:
            continue

        print(f"\nüéß –¢–µ–∫—É—â–∏–π —á–∞–Ω–∫: {current_raw_text}")

        combined_text = buffer_text + " " + current_raw_text
        sentences, buffer_text = extract_sentences(combined_text)

        for sentence in sentences:
            sentence = sentence.strip()
            if sentence and sentence not in full_text:
                capitalized = sentence[0].upper() + sentence[1:] if sentence else ""
                full_text += capitalized + " "
                print(f"‚úÖ –î–æ–±–∞–≤–ª–µ–Ω–æ: {capitalized}")

        if STOP_PHRASE in current_raw_text:
            print("üõë –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Ñ—Ä–∞–∑–∞ –æ—Å—Ç–∞–Ω–æ–≤–∫–∏.")
            stop_event.set()
            break

    print("\nüìã –ü–æ–ª–Ω–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è:")
    final_text = full_text.strip()
    if final_text:
        final_text = final_text[0].upper() + final_text[1:]
    print(final_text)

# --- –ó–∞–ø—É—Å–∫ ---
rec_thread = threading.Thread(target=record_audio)
trans_thread = threading.Thread(target=transcribe_loop)

print("–°–∫–∞–∂–∏—Ç–µ —á—Ç–æ-–Ω–∏–±—É–¥—å... (–°–∫–∞–∂–∏—Ç–µ '—Å—Ç–æ–ø' –¥–ª—è –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è)")
rec_thread.start()
trans_thread.start()

rec_thread.join()
trans_thread.join()
print("‚úÖ –ó–∞–≤–µ—Ä—à–µ–Ω–æ.")
