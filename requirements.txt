# CMAKE_ARGS="-DLLAMA_CUDA=on" pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir
git+https://github.com/m-bain/whisperX.git
bcrypt==4.3.0
passlib==1.7.4
torch==2.7.0+cu118
torchaudio==2.7.0+cu118
torchvision==0.22.0+cu118
python-multipart==0.0.20
python-jose==3.4.0
psycopg2==2.9.10
fastapi==0.115.12
uvicorn==0.34.2
numpy==2.2.5
sounddevice==0.5.2
scipy==1.15.2
python-dotenv==1.1.0