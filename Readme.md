–¢–†–ê–ù–°–ö–†–ò–ë–ê–¶–ò–Ø:


--- –°–∫–∞—á–∏–≤–∞–µ–º –Ω–∞ —Å–≤–æ–π –ü–ö:
1. Python3.10: https://www.python.org/downloads/release/python-31011/
2. NVIDIA CUDA Toolkit: https://developer.nvidia.com/cuda-downloads


--- –°–æ–∑–¥–∞—ë–º –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ:
```sh
python -m venv .venv
```


--- –í—Ö–æ–¥–∏–º –≤ –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ:
–ê) –ß–µ—Ä–µ–∑ –ø–µ—Ä–µ–∫–ª—é—á–µ–Ω–∏–µ –≤–µ—Ä—Å–∏–π python —Å–ø—Ä–∞–≤–∞ –≤–Ω–∏–∑—É VSCode —Å "3.10 Global" –Ω–∞ "3.10 (.venv)"
–ü–æ—Å–ª–µ —ç—Ç–æ–≥–æ –æ—Ç–∫—Ä—ã—Ç—å –Ω–æ–≤—ã–π —Ç–µ—Ä–º–∏–Ω–∞–ª –∏ –¥–æ–ª–∂–Ω–æ –ø–æ—è–≤–∏—Ç—å—Å—è —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ, —á—Ç–æ venv —Å–∫—Ä—ã—Ç–æ –∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞–Ω.
–ë) –ï—Å–ª–∏ –Ω–µ –ø–æ–ª—É—á–∏–ª–æ—Å—å —Å–¥–µ–ª–∞—Ç—å –ê), —Ç–æ –≤—Ç–æ—Ä–æ–π –ø–æ–¥—Ö–æ–¥(aka "–ö–∞–º–µ–Ω–Ω—ã–π –≤–µ–∫"):
**Windows:**
```sh
source .venv/Scripts/activate
#–∏–Ω–æ–≥–¥–∞ –Ω–µ —Ä–∞–±–æ—Ç–∞–µ—Ç –∏ –Ω–∞–¥–æ –±–µ–∑ source –ø—Ä–æ—Å—Ç–æ –Ω–∞–ø–∏—Å–∞—Ç—å: .venv/Scripts/activate
```
**Linux/Mac:**
```sh
source .venv/bin/activate
```


--- Whisper –∏ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏.
–í–ê–†–ò–ê–ù–¢ –ê(—á–∞—Ç –≥–ø—Ç)):
--- –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º Pytorch –ø–µ—Ä–µ–¥ whisperx: https://pytorch.org
```sh
# –ü–æ—á–µ–º—É –∏–º–µ–Ω–Ω–æ —ç—Ç–∞ –≤–µ—Ä—Å–∏—è? —Ç–∏–ø –¥—Ä—É–≥–∏–µ –≤—ã–¥–∞—é—Ç –æ—à–∏–±–∫–∏ –¥–∞, –Ω–æ –≥–¥–µ –Ω–∞–ø–∏—Å–∞–Ω–Ω–æ —á—Ç–æ —ç—Ç–∞ –æ–∫–∞–∂–µ—Ç—Å—è –Ω–æ—Ä–º? –∫–∞–∫ –Ω–∞–≥—É–≥–ª–∏–ª?
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121  # –î–ª—è GPU
```
```sh
# –ü–æ—á–µ–º—É –Ω–µ —á–µ—Ä–µ–∑ pip install whisper
pip install git+https://github.com/m-bain/whisperX.git
```
```sh
pip install numpy torchaudio transformers ffmpeg-python silero-vad
```
```sh
pip install python-dotenv
```
–í–ê–†–ò–ê–ù–¢ –ë(—É –º–µ–Ω—è —Å—Ä–∞–±–æ—Ç–∞–ª–æ):
- –£—Å—Ç–∞–Ω–æ–≤–∫–∞ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ –æ–±—ã—á–Ω–æ–º—É whisper:
1. –°–∫–∞—á–∏–≤–∞–µ–º whisperX:  pip install git+https://github.com/m-bain/whisperX.git
2. —É–¥–∞–ª—è–µ–º torch(–ø–æ—Ç–æ–º—É —á—Ç–æ –µ—Å–ª–∏ —Å–Ω–∞—á–∞–ª–∞ —Å–∫–∞—á–∞—Ç—å —Ç–æ—Ä—á –Ω–µ —Ç–æ–π –≤–µ—Ä—Å–∏–∏, —Ç–æ whisper –ø—Ä–∏ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–∏ —Å–∫–∞—á–µ—Ç —Ç–æ—Ä—á –Ω–∞ cpu):   python -m pip uninstall torch
3. —É—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º torch —Å CUDA:   https://pytorch.org/get-started/locally/


–î–ê–õ–ï–ï:
--- –°–æ–∑–¥–∞—ë–º —Å–µ–±–µ –¢–æ–∫–µ–Ω Hugging Face –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏ –¥–ª—è –¥–∏–∞—Ä–∏–∑–∞—Ü–∏–∏ —Å–ø–∏–∫–µ—Ä–∞:
1. –°–æ–∑–¥–∞—ë–º –∞–∫–∫–∞—É–Ω—Ç –Ω–∞ HuggingFace.
2. –ò–¥—ë–º –≤ –ù–∞—Å—Ç—Ä–æ–π–∫–∏ => Access Tokens(https://huggingface.co/settings/tokens) => Create new token =>
{
Token name = whisperx
–í—Å–µ –≥–∞–ª–æ—á–∫–∏ –æ—Å—Ç–∞–≤—å—Ç–µ –≤ –∏–∑–Ω–∞—á–∞–ª—å–Ω–æ–º —Å–æ—Å—Ç–æ—è–Ω–∏–∏
–í "Repositories permissions" –¥–æ–±–∞–≤—å—Ç–µ: pyannote/speaker-diarization-3.1 –∏ pyannote/segmentation-3.0
}
=> CreateToken => –°–∫–æ–ø–∏—Ä—ã–≤–∞—Ç—å —Ç–æ–∫–µ–Ω –∏ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –µ–≥–æ –∫—É–¥–∞-–Ω–∏–±—É–¥—å.
3. –ü–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –º–æ–¥–µ–ª—è–º. –ù–∞–¥–æ –∑–∞–π—Ç–∏ –Ω–∞ –∫–∞–∂–¥—É—é –∏–∑ —Å—Å—ã–ª–æ–∫ –∏ –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –º–æ–¥–µ–ª—è–º(–¥–ª—è —ç—Ç–æ–≥–æ –∑–∞–ø–æ–ª–Ω–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –≤–≤–µ—Ä—Ö—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã).
3.1 https://huggingface.co/pyannote/segmentation-3.0
3.2 https://huggingface.co/pyannote/speaker-diarization-3.1
4. –°–æ–∑–¥–∞—ë–º –≤ –ø—Ä–æ–µ–∫—Ç–µ –≤ –∫–æ—Ä–Ω–µ–≤–æ–º –∫–∞—Ç–∞–ª–æ–≥–µ —Ñ–∞–π–ª —Å –Ω–∞–∑–≤–∞–Ω–∏–µ–º ".env"
–í—Å—Ç–∞–≤–ª—è–µ–º –≤ –Ω–µ–≥–æ —Å—Ç—Ä–æ–∫—É:
```python
HUGGINGFACE_TOKEN="hf_..."
```
5. –í—Å—Ç–∞–≤—å—Ç–µ –≤ "" —Å–≤–æ–π —Ç–æ–∫–µ–Ω.



--- –£—Å—Ç–∞–Ω–æ–≤–∫–∞ cuDNN
1. –ê–≤—Ç–æ—Ä–∏–∑—É–π—Ç–µ—Å—å –Ω–∞ —Å–∞–π—Ç–µ **NVIDIA**
2. –ü–µ—Ä–µ–π–¥–∏—Ç–µ –≤ —Ä–∞–∑–¥–µ–ª –∞—Ä—Ö–∏–≤–∞ –∏ —Å–∫–∞—á–∞–π—Ç–µ **cuDNN v8.9.6**:
   üîó [–°–∫–∞—á–∞—Ç—å cuDNN v8.9.6](https://developer.nvidia.com/rdp/cudnn-archive)
3. –†–∞—Å–ø–∞–∫—É–π—Ç–µ –∞—Ä—Ö–∏–≤ –∏ —Å–∫–æ–ø–∏—Ä—É–π—Ç–µ —Ñ–∞–π–ª—ã –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—â–∏–µ –ø–∞–ø–∫–∏:
| –ü–∞–ø–∫–∞ –≤ –∞—Ä—Ö–∏–≤–µ | –ö—É–¥–∞ —Å–∫–æ–ø–∏—Ä–æ–≤–∞—Ç—å |
|---------------|----------------|
| **bin/** | `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8\bin` |
| **lib/** | `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8\lib\x64` |
| **include/** | `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.8\include` |





---

## –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∞—É–¥–∏–æ—Ñ–∞–π–ª–∞
–°–∫–æ–ø–∏—Ä—É–π—Ç–µ –∞—É–¥–∏–æ—Ñ–∞–π–ª –≤ —Ä–∞–±–æ—á—É—é –ø–∞–ø–∫—É –∏ —É–∫–∞–∂–∏—Ç–µ –µ–≥–æ –Ω–∞–∑–≤–∞–Ω–∏–µ –≤ –∫–æ–¥–µ **transcriber.py**.

---

## –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∫—Ä–∏–ø—Ç–∞:
–í—ã—Å—Ç–∞–≤—å—Ç–µ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –≤ transcriber.py:
–¢–µ, —á—Ç–æ –≤ –ø–æ–¥–ø—É–Ω–∫—Ç–∞—Ö: "—É—Å—Ç–∞–Ω–æ–≤–∏ –∑–Ω–∞—á–µ–Ω–∏—è –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º" –∏ "–∑–∞–≤–∏—Å–∏—Ç –æ—Ç —É—Å—Ç—Ä–æ–π—Å—Ç–≤–∞".












–ì–ï–ù–ï–†–ê–¶–ò–Ø –ü–†–û–¢–û–ö–û–õ–ê:


(–¥–ª—è –≥–∞–π–¥–∞ –ø–æ –∑–∞–ø—É—Å–∫—É transcriber.py —Å–º–æ—Ç—Ä–∏ –Ω–∞—à –æ—Å–Ω–æ–≤–Ω–æ–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–µ–π)
**–î–ª—è –∑–∞–ø—É—Å–∫–∞ llama_protocol_generator.py –Ω—É–∂–Ω–æ:**
0. –í–∏—Ä—Ç—É–∞–ª—å–Ω–æ–µ –æ–∫—Ä—É–∂–µ–Ω–∏–µ: python -m venv .venv

1. –í –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–º –æ–∫—Ä—É–∂–µ–Ω–∏–∏ –≤ –∫–æ–Ω—Å–æ–ª—å –≤–≤–µ—Å—Ç–∏:
```bash
$env:CMAKE_ARGS="-DLLAMA_CUDA=on" #—Å–æ–∑–¥–∞—Ç—å –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è
pip install llama-cpp-python --force-reinstall --upgrade --no-cache-dir --verbose #—É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å llama-cpp
```
–ë—É–¥–µ—Ç –¥–æ–ª–≥–æ–µ —Å–∫–∞—á–∏–≤–∞–Ω–∏–µ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞(–º–∏–Ω—É—Ç 10-15)

2. –°–∫–∞—á–∞—Ç—å –º–æ–¥–µ–ª—å —Å —Å–∞–π—Ç–∞.
–ó–∞—Ö–æ–¥–∏—Ç–µ –Ω–∞ —Å–∞–π—Ç https://huggingface.co/mradermacher/llama-3-8B-Instruct-GGUF 
–∏ –≤ —Ç–∞–±–ª–∏—Ü–µ –≤—ã–±–∏—Ä–∞–µ—Ç–µ –º–æ–¥–µ–ª—å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è ([—Ç–∞–±–ª–∏—Ü–∞ —Å —Å–∞–π—Ç–∞](https://huggingface.co/mradermacher/llama-3-8B-Instruct-GGUF#provided-quants))

–ú–æ–∂–Ω–æ —ç–∫—Å–ø–µ—Ä–µ–º–µ–Ω–∏—Ç—Ä–æ–≤–∞—Ç—å —Å —Ä–∞–∑–Ω—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏. –ú—ã –≤—ã–±—Ä–∞–ª–∏ –¥–ª—è –Ω–∞—á–∞–ª–∞ –∏–∑ —Ä–µ–∫–æ–º–µ–Ω–¥–æ–≤–∞–Ω–Ω—ã—Ö —ç—Ç—É: Q4_K_M	

3. –°–∫–∞—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å –ø–µ—Ä–µ–º–µ—Å—Ç–∏—Ç—å –≤ –ø–∞–ø–∫—É models –≤ –∫–æ—Ä–Ω–µ–≤–æ–º –∫–∞—Ç–∞–ª–æ–≥–µ.

4. –í —Å–∫—Ä–∏–ø—Ç–µ llama_protocol_generator.py –Ω–∞–¥–æ –ø–æ–±–∞–ª–æ–≤–∞—Ç—å—Å—è —Å –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏.
llm = Llama(
    model_path="models/llama-3-8B-Instruct.Q4_K_M.gguf",
    n_ctx=16384,              # –£–±–µ–¥–∏—Å—å, —á—Ç–æ –≤–µ—Å—å —Ç–µ–∫—Å—Ç –ø–æ–º–µ—â–∞–µ—Ç—Å—è –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç
    n_gpu_layers=35,         # –ü–æ–¥ —Ç–≤–æ—é RTX 4060 Ti (8 GB)
    n_threads=6,
    chat_format="llama-3"
)
- –∫–∞–∫ –º–∏–Ω–∏–º—É–º —É–∫–∞–∑–∞—Ç—å –≤–µ—Ä—Å–∏—é —Å–∫–∞—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏(–µ—Å–ª–∏ –≤—ã —Å–∫–∞—á–∞—Ç—å –æ—Ç–ª–∏—á–Ω—É—é –æ—Ç —É–∫–∞–∑–∞–Ω–Ω–æ–π –≤ —Å–∫—Ä–∏–ø—Ç–µ)
- –≤ –∑–∞–≤–∏—Å–∏–º—Å–æ—Ç–∏ –æ—Ç —Ä–∞–∑–º–µ—Ä–∞ —Ç–µ–∫—Å—Ç–∞ –ø–æ–Ω–∞–¥–æ–±–∏—Ç—Å—è –∏–∑–º–µ–Ω–∏—Ç—å context(n_ctx): –±–æ–ª—å—à–µ –∫–æ–Ω—Ç–µ–∫—Å—Ç - –±–æ–ª—å—à–µ —Å–ª–æ–≤ –º–æ–∂–µ—Ç –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –≤ –∑–∞–ø—Ä–æ—Å–µ, –Ω–æ —É–º–µ–Ω—å—à–∞–µ—Ç—Å—è —Å–∫–æ—Ä–æ—Å—Ç—å.

5. –í TEXT_after_transcrption –ø–æ–º–µ—Å—Ç–∏—Ç–µ –≤–∞—à —Ç–µ–∫—Å—Ç –ø–æ—Å–ª–µ —Ç—Ä–∞–Ω—Å–∫—Ä–∏–±–∞—Ü–∏–∏ –≤ transcription_oitput.txt

6. –ò–º–∑–µ–Ω–∏—Ç—å –ø—Ä–æ–º–ø—Ç –≤–Ω—É—Ç—Ä–∏ —Å–∫—Ä–∏–ø—Ç–∞ –¥–ª—è –≤–∞—à–∏—Ö —Ü–µ–ª–µ–π.
–ì–æ—Ç–æ–≤–æ. –ó–∞–ø—É—Å–∫–∞–π—Ç–µ llama_protocol_generator.py

